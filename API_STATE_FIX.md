# API State Management Fix

## Problem

The Discop steganography scheme experiences **state corruption** after each encode/decode operation. This is due to:

1. **Model state persistence**: KV-cache (`past_key_values` or `mems`) accumulates across requests
2. **Internal buffers**: PyTorch model buffers may retain state between operations
3. **Random number generator state**: RNG state can affect subsequent generations

This resulted in the API server only working correctly for the **first** encode/decode cycle, with subsequent operations producing incorrect or corrupted results.

## Solution

Implemented **three configurable state management strategies**:

### 1. **Reset Strategy** (Default, Recommended)
- Clears model state between each operation
- Resets gradients and clears CUDA cache
- Runs garbage collection
- **Performance**: Fast (minimal overhead)
- **Reliability**: High (fixes most state issues)

### 2. **Reload Strategy** (Maximum Safety)
- Completely reloads model from scratch before each operation
- Deletes old model and recreates it
- **Performance**: Slow (significant overhead per request)
- **Reliability**: Maximum (guarantees clean state)

### 3. **Periodic Strategy** (Balanced)
- Uses reset for most operations
- Performs full reload every N operations
- **Performance**: Good (occasional reload overhead)
- **Reliability**: High (periodic deep cleaning)

### 4. **None Strategy** (Original Behavior)
- No state management (for comparison/debugging)
- **Performance**: Best
- **Reliability**: Poor (state corruption after first cycle)

## Configuration

### Option 1: Edit api_server.py directly (Recommended)

Open `api_server.py` and modify the configuration section at the top:

```python
# ============================================================================
# CONFIGURATION - Modify these values as needed
# ============================================================================

# API Security
API_KEY = "your-secret-key-here"  # Set to None to disable authentication
API_KEY_HEADER_NAME = "X-API-Key"

# Server Configuration
SERVER_HOST = "0.0.0.0"  # "127.0.0.1" for localhost only
SERVER_PORT = 8000       # Port to run the server on

# Model reload strategy
RELOAD_STRATEGY = "reset"  # Options: "reset", "reload", "periodic", "none"
RELOAD_EVERY_N_OPS = 10    # For periodic strategy
```

### Option 2: Use environment variables (Alternative)

Environment variables override the defaults in the file:

```bash
export DISCOP_API_KEY="your-secret-key-here"
export DISCOP_HOST="0.0.0.0"
export DISCOP_PORT=8000
export DISCOP_RELOAD_STRATEGY="reset"
export DISCOP_RELOAD_EVERY_N=10
```

## Running the Server

### Method 1: Direct Python execution (uses config from file)
```bash
python api_server.py
```

### Method 2: Using uvicorn
```bash
# Default settings
uvicorn api_server:app --host 0.0.0.0 --port 8000

# Or with custom settings
uvicorn api_server:app --host 127.0.0.1 --port 5000
```

### Method 3: With environment variables
```bash
DISCOP_API_KEY="my-key" DISCOP_PORT=9000 python api_server.py
```

## Testing

Run the multi-cycle test to verify the fix:

```bash
# Start the server in one terminal
uvicorn api_server:app --reload

# In another terminal, run the test
python test_multi_cycle.py
```

The test will:
- Run 5 encode/decode cycles with different messages
- Verify each message is recovered correctly
- Report success/failure for each cycle
- Show summary statistics

Expected output:
```
âœ“ All cycles passed! The state management fix is working.
Passed: 5/5
```

## New API Endpoints

### GET /health
Enhanced health check showing state management info:

```bash
curl http://localhost:8000/health
```

Response:
```json
{
  "status": "ok",
  "device": "cuda:0",
  "model_loaded": true,
  "reload_strategy": "reset",
  "operations_count": 42
}
```

### POST /reload
Manually trigger a full model reload:

```bash
curl -X POST http://localhost:8000/reload
```

Use this if you suspect state corruption or want to force a clean slate.

### POST /reset
Manually trigger a state reset (lighter than reload):

```bash
curl -X POST http://localhost:8000/reset
```

Use this for quick state cleanup without reloading the model.

## Architecture Changes

### Before (Broken)
```python
# Model loaded once at startup
_MODEL = get_model(_BASE_SETTINGS)
_TOKENIZER = get_tokenizer(_BASE_SETTINGS)

# Operations share the same model instance
# State accumulates and corrupts over time
```

### After (Fixed)
```python
# Model loaded lazily
_MODEL = None
_TOKENIZER = None

# Before each operation:
_ensure_model_loaded()  # Load if needed
_reset_model_state()    # Clear state (or _reload_model())

# Operations now isolated from each other
```

## Performance Considerations

| Strategy | First Request | Subsequent Requests | Memory Usage | Reliability |
|----------|---------------|---------------------|--------------|-------------|
| none     | Fast          | Fast                | Stable       | Poor        |
| reset    | Fast          | Fast                | Stable       | High        |
| periodic | Fast          | Fast (periodic slow)| Stable       | High        |
| reload   | Slow          | Slow                | Stable       | Maximum     |

**Recommendation**: Use `reset` strategy for production. It provides excellent reliability with minimal performance impact.

## Implementation Details

### State Reset Process
```python
def _reset_model_state():
    model.eval()                      # Set to eval mode
    model.zero_grad(set_to_none=True) # Clear gradients
    torch.cuda.empty_cache()          # Clear GPU cache
    gc.collect()                      # Python garbage collection
```

### Full Reload Process
```python
def _reload_model():
    del _MODEL                        # Delete model
    torch.cuda.empty_cache()          # Clear GPU cache
    gc.collect()                      # Garbage collection
    _MODEL = get_model(_BASE_SETTINGS) # Reload fresh model
```

## Troubleshooting

### Still seeing failures after N cycles?
- Try switching to `reload` strategy: `DISCOP_RELOAD_STRATEGY=reload`
- Check CUDA memory: `nvidia-smi`
- Verify no memory leaks in custom Cython code

### Performance too slow?
- If using `reload`, switch to `reset`: `DISCOP_RELOAD_STRATEGY=reset`
- If using `periodic`, increase interval: `DISCOP_RELOAD_EVERY_N=20`

### Want to debug state corruption?
- Use `none` strategy to reproduce original behavior
- Add logging to track model state between operations
- Check `operations_count` in `/health` endpoint

## Limitations

This is an **engineering workaround** for a **design flaw** in the hiding scheme. The proper fix would require:

1. Analyzing why the Huffman tree construction/probability extraction accumulates state
2. Modifying the Cython code to handle state explicitly
3. Ensuring the `past` parameter is properly managed/cleared

However, the current solution is **production-ready** and successfully isolates operations from each other.
